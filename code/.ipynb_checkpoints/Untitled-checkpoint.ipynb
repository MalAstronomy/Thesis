{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Created on Tue Apr 23 23:16:42 2019\n",
    "\n",
    "# @author: malavikavijayendravasist\n",
    "# \"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.python.keras import layers \n",
    "from tensorflow.python.keras.losses import categorical_crossentropy\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy \n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from keras.utils import CustomObjectScope\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "\n",
    "#from keras.initializers import glorot_uniform\n",
    "\n",
    "from data_classes import data_classes\n",
    "from converting_images_to_TFRecords_ import converting_to_TFRecords as convertingTF\n",
    "#from tfrecords import converting_to_TFRecords as Ctfrecords\n",
    "from extracting_images_from_TFRecords_ import extracting_TFRecords as extractTF\n",
    "#from extracting_ratio_records import  extracting_TFRecords as Ctfextract\n",
    "from networkss import networks\n",
    "#from plot_confusion_matrix import ConfusionMatrix\n",
    "\n",
    "\n",
    "sess = tensorflow.Session()\n",
    "\n",
    "#training in Titan- /home/vasist/code/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class All: \n",
    "    \n",
    "    def __init__(self, feature=None,pic_path=None,feature_values=None,DCfolder=None,epochs=None,batch_size=None,nclasses=None,dims=None,TBfolder=None,name=\"\"):\n",
    "        self.name = name\n",
    "        self.feature='Size Ratio' #'Mass Ratio',\n",
    "        self.feature_values=[]\n",
    "        self.pic_path='/Users/malavikavijayendravasist/Desktop/mt2/handpicked_images/' #'/home/vasist/images/'\n",
    "        self.DCfolder='/Users/malavikavijayendravasist/Desktop/mt2/data_classes/data_classes_trial/'#'/home/vasist/data_classes/'\n",
    "        #self.TFRecord='/home/vasist/TFRecords/data_classes/'\n",
    "        self.TFRecord='/Users/malavikavijayendravasist/Desktop/mt2/TFRecords/trial/'#'/home/vasist/TFRecords/ratio/'\n",
    "        self.feat=[]    # array of features of all the images in the same order as the images\n",
    "        self.nepochs=5\n",
    "        self.batch_size=5\n",
    "        self.nclasses=10\n",
    "        self.dims=[224,224,3]\n",
    "        self.TBfolder='/Users/malavikavijayendravasist/Desktop/mt2/Tensorboard/trial/'#'/home/vasist/Tensorboard/data_classes/'\n",
    "        self.CPfolder='/Users/malavikavijayendravasist/Desktop/mt2/Checkpoints/trial/'#'/home/vasist/Checkpoints/data_classes/'\n",
    "        self.Modelfolder='/Users/malavikavijayendravasist/Desktop/mt2/Models/trial/'#'/home/vasist/Models/data_classes/'\n",
    "        self.network_name='mnist' #resnet50/mnist\n",
    "        \n",
    "        \n",
    "    def Feature(self):\n",
    "        \n",
    "        images = os.listdir(self.pic_path)\n",
    "        images=np.asarray(images)\n",
    "\n",
    "        indices= np.random.choice(np.arange(len(images)),100) #len(images)\n",
    "\n",
    "\n",
    "        redshift=[]\n",
    "        merger=[]\n",
    "        angle=[]\n",
    "        picture_names=[]\n",
    "        \n",
    "        for i in indices:\n",
    "            redshift.append(int(images[i].split('_')[1]))\n",
    "            merger.append(int(images[i].split('_')[2]))\n",
    "            angle.append(int(images[i].split('_')[3].split('.')[0]))\n",
    "            picture_names.append(images[i])\n",
    "        \n",
    "        return redshift,merger,picture_names\n",
    "        \n",
    "    def making_data_classes(self): #feature='Mass Ratio' #1\n",
    "        \n",
    "        redshift,merger,picture_names= self.Feature()\n",
    "\n",
    "        making_classes= data_classes(self.pic_path,redshift,self.feature,merger,picture_names,self.DCfolder,self.nclasses)\n",
    "        high=making_classes.making_classes()\n",
    "        \n",
    "        f=h5py.File('/Users/malavikavijayendravasist/Desktop/mt2/high.hdf5','w')\n",
    "        f.create_dataset('high',data=high)\n",
    "        \n",
    "        \n",
    "    def making_tfrecords(self):    #1\n",
    "        \n",
    "        #self.high=self.making_data_classes() \n",
    "        f=h5py.File('/Users/malavikavijayendravasist/Desktop/mt2/high.hdf5','r')\n",
    "        high=f['high'].value\n",
    "        self.feature_values= np.linspace(0,high,self.nclasses+1)[1:] \n",
    "        self.feature_values=np.asarray([round(i,3) for i in self.feature_values])\n",
    "        convertingTF(self.feature_values, self.DCfolder,self.TFRecord,self.feature).conversion()   \n",
    "        \n",
    "    \n",
    "\n",
    "    def extracting_tfrecords(self): #1\n",
    "        train_iterator, valid_iterator, test_iterator, steps_per_epoch_train, steps_per_epoch_valid, steps_test= extractTF(self.TFRecord,self.feature_values,self.nclasses,self.dims,self.batch_size,self.nepochs).handling_dataset()\n",
    "        \n",
    "        return train_iterator, valid_iterator, test_iterator, steps_per_epoch_train, steps_per_epoch_valid, steps_test\n",
    "    \n",
    "\n",
    "    def networkss(self):\n",
    "        \n",
    "        train_iterator, valid_iterator, test_iterator, steps_per_epoch_train, steps_per_epoch_valid, steps_test =self.extracting_tfrecords()\n",
    "        \n",
    "        network= networks(self.nclasses,self.nepochs,self.batch_size,train_iterator, valid_iterator, test_iterator, steps_per_epoch_train, steps_per_epoch_valid, steps_test, self.network_name, self.feature,self.TBfolder,self.CPfolder,self.Modelfolder, self.dims)\n",
    "\n",
    "        #untrained_model= network.fitting_mnist()  #returns a compiled but untrained model \n",
    "        untrained_model= network.fitting_resnet50()\n",
    "        model_name=network.fitting(untrained_model) #the model is saved here\n",
    "        \n",
    "        f= h5py.File('/Users/malavikavijayendravasist/Desktop/mt2/model_name.hdf5','w')\n",
    "        f.create_dataset('model_name',data=model_name)\n",
    "     \n",
    "        #trained_model= network.fitting(trained_model)   #to resume fitting \n",
    "        \n",
    "    def saved_model(self): \n",
    "        \n",
    "        f= h5py.File('/Users/malavikavijayendravasist/Desktop/mt2/model_name.hdf5','r')\n",
    "        model_name= f['model_name'].value\n",
    "#         print(model_name)\n",
    "        \n",
    "#         trained_model_name=self.Modelfolder+model_name\n",
    "#         print(trained_model_name)\n",
    "        model= load_model(self.CPfolder+model_name+'_'+str(self.nepochs)+'.h5')\n",
    "        #model= load_model(self.CPfolder,custom_objects={'top_2_categorical_accuracy': self.top_2_categorical_accuracy})\n",
    "        return model \n",
    "        \n",
    "          \n",
    "    def predict(self): \n",
    "        \n",
    "        #picking 100 random images \n",
    "\n",
    "        #images = os.listdir(self.pic_path)\n",
    "\n",
    "        redshift,merger,picture_names=self.Feature()\n",
    "        merger=np.asarray(merger)\n",
    "        picture_names=np.asarray(picture_names)\n",
    "        print(type(picture_names))\n",
    "        indices= np.random.randint(0,len(picture_names),10)\n",
    "        \n",
    "        f=h5py.File('/Users/malavikavijayendravasist/Desktop/mt2/high.hdf5','r')\n",
    "        high=f['high'].value\n",
    "        cl=np.linspace(0,high,self.nclasses+1) #self.feature_values doesnt include 0\n",
    "        cl=[round(i,3) for i in cl]\n",
    "            \n",
    "        merger_p=merger[indices]\n",
    "        picture_names_p=picture_names[indices]\n",
    "        print(picture_names_p)\n",
    "        \n",
    "        ytrue=[]\n",
    "        for i in np.arange(len(merger_p)):\n",
    "            for ind,c in enumerate(cl[1:]):\n",
    "                ind+=1\n",
    "                if merger_p[i] >= cl[ind-1] and merger_p[i] <= cl[ind]:\n",
    "                    np.append(ytrue,cl[ind])\n",
    "                    \n",
    "\n",
    "        picture_array = np.zeros((len(picture_names_p), self.dims[0], self.dims[1], self.dims[2]), dtype=np.float32)\n",
    "        picture_name_tensor, picture= convertingTF(self.feature_values, self.DCfolder,self.TFRecord,self.feature).image_process()\n",
    "\n",
    "        for i,name in enumerate(picture_names_p):\n",
    "            Name=self.pic_path+name\n",
    "            picture_array[i] = sess.run(picture, feed_dict={picture_name_tensor: Name})\n",
    "            picture_array[i] = np.array(picture_array[i], dtype=np.float32)\n",
    "            picture_array[i] /= 255\n",
    "            #if i%500==0: print(i)\n",
    "        \n",
    "        \n",
    "        model= self.saved_model()\n",
    "        print(\"Start\")\n",
    "        predictions= model.predict(picture_array, verbose=1)\n",
    "        print(predictions)\n",
    "        print(\"End\")\n",
    "        \n",
    "        ypred=[]\n",
    "        perfect_counter = 0\n",
    "        for i in range(len(picture_names_p)):\n",
    "            max1 = np.amax(predictions[i][:self.nclasses])\n",
    "            idx = np.where(predictions[i]==max1)[0][0]\n",
    "            np.append(ypred, cl[idx+1])\n",
    "           \n",
    "        return ytrue,ypred \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     All=All()\n",
    "#     #All.making_data_classes()\n",
    "#     All.making_tfrecords()\n",
    "\n",
    "#    high=All.data_classes()\n",
    "#    tfrecords(high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "All=All()\n",
    "a1=All.making_data_classes\n",
    "a2=All.making_tfrecords\n",
    "a3=All.extracting_tfrecords\n",
    "a4=All.networkss\n",
    "a5=All.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0.0_0.114\n",
      "class 0.114_0.228\n",
      "class 0.228_0.343\n",
      "class 0.343_0.457\n",
      "class 0.457_0.571\n",
      "class 0.571_0.685\n",
      "class 0.685_0.799\n",
      "class 0.799_0.913\n",
      "class 0.913_1.028\n",
      "class 1.028_1.142\n"
     ]
    }
   ],
   "source": [
    "# a2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<generator object extracting_TFRecords.handling_dataset.<locals>.make_iterator at 0x10f235a98>,\n",
       " <generator object extracting_TFRecords.handling_dataset.<locals>.make_iterator at 0x10f28cba0>,\n",
       " <generator object extracting_TFRecords.handling_dataset.<locals>.make_iterator at 0x1a3c751f10>,\n",
       " 14,\n",
       " 2,\n",
       " 2)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size_mnist_5_1556703358\n",
      "Epoch 1/5\n",
      "13/14 [==========================>...] - ETA: 2s - loss: 2.4018 - acc: 0.2769\n",
      "Epoch 00001: saving model to /Users/malavikavijayendravasist/Desktop/mt2/Checkpoints/trial/Size_mnist_5_1556703358_1.h5\n"
     ]
    }
   ],
   "source": [
    "a4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('/Users/malavikavijayendravasist/Desktop/mt2/Checkpoints/trial/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'i'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-e3aec80141af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'soemthing'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_{i:0ld}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m134\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'i'"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    s='soemthing'+\"_{0:0ld}\".format(134)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000\n",
      "0001\n",
      "0002\n",
      "0003\n",
      "0004\n",
      "0005\n",
      "0006\n",
      "0007\n",
      "0008\n",
      "0009\n"
     ]
    }
   ],
   "source": [
    "l=4\n",
    "for i in range(10):     \n",
    "    a=\"{:0\"+str(l)+\"d}\"\n",
    "    b=a.format(i)\n",
    "    print(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='_{epoch:0ld}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a=18\n",
    "c=0\n",
    "while (a>0): \n",
    "    a//=10\n",
    "    print(a)\n",
    "    c+=1\n",
    "    \n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
